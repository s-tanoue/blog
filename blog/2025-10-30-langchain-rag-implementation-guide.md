---
slug: langchain-rag-implementation-guide
title: LangChain ã§ç¤¾å†…ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã™ã‚‹ï¼šRAG ã®å®Œå…¨å®Ÿè£…ã‚¬ã‚¤ãƒ‰
authors:
  name: Blog Maintainer
  title: Maintainer
  url: https://example.com
  image_url: https://avatars.githubusercontent.com/u/1?v=4
categories:
  - AIãƒ»LLM
---

ç¤¾å†…ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚„ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã‹ã‚‰æ­£ç¢ºãªæƒ…å ±ã‚’æ¤œç´¢ã—ã€è‡ªç„¶è¨€èªã§å›ç­”ã‚’ç”Ÿæˆã™ã‚‹RAGï¼ˆRetrieval-Augmented Generationï¼‰ã‚·ã‚¹ãƒ†ãƒ ã¯ã€ç¾ä»£ã®ä¼æ¥­ã«ã¨ã£ã¦å¿…é ˆã®ãƒ„ãƒ¼ãƒ«ã¨ãªã£ã¦ã„ã¾ã™ã€‚ã“ã®è¨˜äº‹ã§ã¯ã€LangChainã‚’ä½¿ã£ã¦æœ¬æ ¼çš„ãªRAGã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã™ã‚‹æ–¹æ³•ã‚’ã€**ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢**ã€**ãƒãƒ£ãƒ³ã‚¯æˆ¦ç•¥**ã€**ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­è¨ˆ**ã®3ã¤ã®é‡è¦ãªè¦ç´ ã«ç„¦ç‚¹ã‚’å½“ã¦ã¦è§£èª¬ã—ã¾ã™ã€‚

<!--truncate-->

## RAGã‚·ã‚¹ãƒ†ãƒ ã¨ã¯

RAGï¼ˆRetrieval-Augmented Generationï¼‰ã¯ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã®èƒ½åŠ›ã‚’æ‹¡å¼µã—ã€ç‰¹å®šã®ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ã‚„æœ€æ–°æƒ…å ±ã«åŸºã¥ã„ãŸå›ç­”ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã®æŠ€è¡“ã§ã™ã€‚

### RAGã®ä»•çµ„ã¿

```
1. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®æº–å‚™
   â†“
2. ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
   â†“
3. ãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼ˆã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ï¼‰
   â†“
4. ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã«ä¿å­˜
   â†“
5. ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–
   â†“
6. é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æ¤œç´¢
   â†“
7. æ¤œç´¢çµæœã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦LLMã«æ¸¡ã™
   â†“
8. å›ç­”ã‚’ç”Ÿæˆ
```

### RAGã®åˆ©ç‚¹

- **ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆå¹»è¦šï¼‰ã®å‰Šæ¸›**: ä¿¡é ¼ã§ãã‚‹ã‚½ãƒ¼ã‚¹ã«åŸºã¥ã„ãŸå›ç­”
- **æœ€æ–°æƒ…å ±ã®æä¾›**: å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®æœŸé™ã‚’è¶…ãˆãŸæƒ…å ±ã«ã‚¢ã‚¯ã‚»ã‚¹
- **ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºå¯èƒ½**: ä¼æ¥­å›ºæœ‰ã®çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã«å¯¾å¿œ
- **å‡ºå…¸ã®æ˜ç¤º**: ã©ã®æ–‡æ›¸ã‹ã‚‰æƒ…å ±ã‚’å–å¾—ã—ãŸã‹è¿½è·¡å¯èƒ½

---

## ã‚·ã‚¹ãƒ†ãƒ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

ä»Šå›æ§‹ç¯‰ã™ã‚‹RAGã‚·ã‚¹ãƒ†ãƒ ã®å…¨ä½“åƒï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. ãƒ‡ãƒ¼ã‚¿æº–å‚™ãƒ•ã‚§ãƒ¼ã‚º                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ â†’ ãƒ­ãƒ¼ãƒ€ãƒ¼ â†’ ãƒãƒ£ãƒ³ã‚¯åˆ†å‰² â†’ ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°  â”‚
â”‚  (PDF/MD/TXT)   (Loaders)   (Splitters)   (Embeddings)  â”‚
â”‚                                    â†“                     â”‚
â”‚                            ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢                 â”‚
â”‚                            (FAISS/Chroma)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. æ¤œç´¢ãƒ»å›ç­”ç”Ÿæˆãƒ•ã‚§ãƒ¼ã‚º                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ãƒ¦ãƒ¼ã‚¶ãƒ¼è³ªå• â†’ ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚° â†’ é¡ä¼¼åº¦æ¤œç´¢              â”‚
â”‚       â†“                              â†“                  â”‚
â”‚   ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ â† é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ                          â”‚
â”‚       â†“                                                 â”‚
â”‚     LLM â†’ å›ç­”ç”Ÿæˆ                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ç’°å¢ƒã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

### å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
# LangChainã®ã‚³ã‚¢ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸
pip install langchain langchain-openai langchain-community

# ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ­ãƒ¼ãƒ€ãƒ¼
pip install pypdf python-docx

# ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢
pip install faiss-cpu chromadb

# ãã®ä»–ã®ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
pip install tiktoken
```

### ç’°å¢ƒå¤‰æ•°ã®è¨­å®š

```python
import os

# OpenAI APIã‚­ãƒ¼
os.environ["OPENAI_API_KEY"] = "your-openai-api-key"

# ã‚ªãƒ—ã‚·ãƒ§ãƒ³: LangSmithã§ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ã‚’æœ‰åŠ¹åŒ–
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_API_KEY"] = "your-langsmith-api-key"
os.environ["LANGCHAIN_PROJECT"] = "rag-system"
```

---

## 1. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒ­ãƒ¼ãƒ‰ã¨æº–å‚™

### å„ç¨®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®èª­ã¿è¾¼ã¿

```python
from langchain_community.document_loaders import (
    TextLoader,
    PyPDFLoader,
    DirectoryLoader,
    UnstructuredMarkdownLoader
)

# ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
def load_text_documents(file_path):
    loader = TextLoader(file_path, encoding='utf-8')
    return loader.load()

# PDFãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
def load_pdf_documents(file_path):
    loader = PyPDFLoader(file_path)
    return loader.load()

# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®å…¨Markdownãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
def load_markdown_directory(directory_path):
    loader = DirectoryLoader(
        directory_path,
        glob="**/*.md",
        loader_cls=UnstructuredMarkdownLoader
    )
    return loader.load()

# å®Ÿéš›ã®ä½¿ç”¨ä¾‹
documents = []
documents.extend(load_markdown_directory("./knowledge_base"))
documents.extend(load_pdf_documents("./manuals/user_guide.pdf"))

print(f"èª­ã¿è¾¼ã‚“ã ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {len(documents)}")
```

### ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç®¡ç†

ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¯æ¤œç´¢çµæœã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚„å‡ºå…¸ã®ç‰¹å®šã«é‡è¦ã§ã™ï¼š

```python
from langchain.schema import Document

# ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æŒã¤ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ä½œæˆ
doc = Document(
    page_content="è£½å“ã®ä¿è¨¼æœŸé–“ã¯è³¼å…¥æ—¥ã‹ã‚‰1å¹´é–“ã§ã™ã€‚",
    metadata={
        "source": "user_manual.pdf",
        "page": 15,
        "category": "warranty",
        "last_updated": "2024-01-15"
    }
)
```

---

## 2. ãƒãƒ£ãƒ³ã‚¯æˆ¦ç•¥ï¼šãƒ†ã‚­ã‚¹ãƒˆã®åˆ†å‰²

ãƒãƒ£ãƒ³ã‚¯æˆ¦ç•¥ã¯RAGã‚·ã‚¹ãƒ†ãƒ ã®å“è³ªã«å¤§ããå½±éŸ¿ã—ã¾ã™ã€‚é©åˆ‡ãªãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã¨åˆ†å‰²æ–¹æ³•ã‚’é¸ã¶ã“ã¨ãŒé‡è¦ã§ã™ã€‚

### ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã®é¸æŠåŸºæº–

| ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º | ç”¨é€” | ãƒ¡ãƒªãƒƒãƒˆ | ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ |
|---------------|------|---------|-----------|
| 100-200ãƒˆãƒ¼ã‚¯ãƒ³ | çŸ­ã„å®šç¾©ã€FAQ | æ¤œç´¢ç²¾åº¦ãŒé«˜ã„ | æ–‡è„ˆãŒä¸è¶³ã—ãŒã¡ |
| 500-1000ãƒˆãƒ¼ã‚¯ãƒ³ | ä¸€èˆ¬çš„ãªæ–‡æ›¸ | ãƒãƒ©ãƒ³ã‚¹ãŒè‰¯ã„ | æ¨™æº–çš„ãªé¸æŠ |
| 1500-2000ãƒˆãƒ¼ã‚¯ãƒ³ | æŠ€è¡“æ–‡æ›¸ã€ãƒ¬ãƒãƒ¼ãƒˆ | æ–‡è„ˆãŒè±Šå¯Œ | æ¤œç´¢ç²¾åº¦ãŒä¸‹ãŒã‚‹ |

### CharacterTextSplitterï¼šã‚·ãƒ³ãƒ—ãƒ«ãªåˆ†å‰²

```python
from langchain.text_splitter import CharacterTextSplitter

# åŸºæœ¬çš„ãªæ–‡å­—æ•°ãƒ™ãƒ¼ã‚¹ã®åˆ†å‰²
text_splitter = CharacterTextSplitter(
    separator="\n\n",           # æ®µè½ã§åˆ†å‰²
    chunk_size=1000,            # ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºï¼ˆæ–‡å­—æ•°ï¼‰
    chunk_overlap=200,          # ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ï¼ˆæ–‡å­—æ•°ï¼‰
    length_function=len,
)

texts = text_splitter.split_documents(documents)
print(f"åˆ†å‰²å¾Œã®ãƒãƒ£ãƒ³ã‚¯æ•°: {len(texts)}")
```

### RecursiveCharacterTextSplitterï¼šæ¨å¥¨ã•ã‚Œã‚‹æ–¹æ³•

éšå±¤çš„ã«åŒºåˆ‡ã‚Šæ–‡å­—ã‚’è©¦ã¿ã‚‹ãŸã‚ã€ã‚ˆã‚Šè‡ªç„¶ãªåˆ†å‰²ãŒå¯èƒ½ã§ã™ï¼š

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

# éšå±¤çš„ã«åˆ†å‰²ã‚’è©¦ã¿ã‚‹
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    length_function=len,
    separators=[
        "\n\n",      # ã¾ãšæ®µè½ã§åˆ†å‰²ã‚’è©¦ã¿ã‚‹
        "\n",        # æ¬¡ã«è¡Œã§åˆ†å‰²
        " ",         # ãã‚Œã§ã‚‚ãƒ€ãƒ¡ãªã‚‰å˜èªã§åˆ†å‰²
        ""           # æœ€å¾Œã®æ‰‹æ®µã¨ã—ã¦æ–‡å­—ã§åˆ†å‰²
    ]
)

texts = text_splitter.split_documents(documents)
```

**æ¨å¥¨è¨­å®šï¼ˆæ—¥æœ¬èªã®å ´åˆï¼‰**:
```python
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,           # æ—¥æœ¬èªã®å ´åˆã€æ–‡å­—æ•°ãƒ™ãƒ¼ã‚¹
    chunk_overlap=200,         # 20%ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—
    separators=[
        "\n\n",
        "\n",
        "ã€‚",                   # æ—¥æœ¬èªã®å¥ç‚¹
        "ã€",                   # æ—¥æœ¬èªã®èª­ç‚¹
        " ",
        ""
    ]
)
```

### TokenTextSplitterï¼šãƒˆãƒ¼ã‚¯ãƒ³ãƒ™ãƒ¼ã‚¹ã®åˆ†å‰²

LLMã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã‚’æ­£ç¢ºã«åˆ¶å¾¡ã—ãŸã„å ´åˆã«ä½¿ç”¨ï¼š

```python
from langchain.text_splitter import TokenTextSplitter

# ãƒˆãƒ¼ã‚¯ãƒ³æ•°ãƒ™ãƒ¼ã‚¹ã§åˆ†å‰²
text_splitter = TokenTextSplitter(
    chunk_size=500,            # ãƒˆãƒ¼ã‚¯ãƒ³æ•°
    chunk_overlap=50
)

texts = text_splitter.split_documents(documents)
```

### ãƒãƒ£ãƒ³ã‚¯ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã®é‡è¦æ€§

ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ãŒãªã„ã¨ã€é‡è¦ãªæƒ…å ±ãŒãƒãƒ£ãƒ³ã‚¯ã®å¢ƒç•Œã§åˆ†æ–­ã•ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ï¼š

```
ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ãªã—:
Chunk 1: "...è£½å“ã®ä¿è¨¼æœŸé–“ã¯"
Chunk 2: "è³¼å…¥æ—¥ã‹ã‚‰1å¹´é–“ã§ã™..."
â†’ ã©ã¡ã‚‰ã®ãƒãƒ£ãƒ³ã‚¯ã‚‚ä¸å®Œå…¨

ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚ã‚Š:
Chunk 1: "...è£½å“ã®ä¿è¨¼æœŸé–“ã¯è³¼å…¥æ—¥ã‹ã‚‰1å¹´é–“ã§ã™..."
Chunk 2: "...ä¿è¨¼æœŸé–“ã¯è³¼å…¥æ—¥ã‹ã‚‰1å¹´é–“ã§ã™ã€‚ä¿®ç†ä¾é ¼ã¯..."
â†’ ä¸¡æ–¹ã®ãƒãƒ£ãƒ³ã‚¯ã§å®Œå…¨ãªæƒ…å ±ãŒå¾—ã‚‰ã‚Œã‚‹
```

**æ¨å¥¨ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ç‡**: ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã®10-20%

### ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°ï¼ˆé«˜åº¦ãªæ‰‹æ³•ï¼‰

æ„å‘³çš„ãªã¾ã¨ã¾ã‚Šã§ãƒãƒ£ãƒ³ã‚¯ã‚’åˆ†å‰²ã™ã‚‹æ‰‹æ³•ï¼š

```python
from langchain_experimental.text_splitter import SemanticChunker
from langchain_openai import OpenAIEmbeddings

# ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ãªå¢ƒç•Œã§åˆ†å‰²
semantic_splitter = SemanticChunker(
    OpenAIEmbeddings(),
    breakpoint_threshold_type="percentile",  # åˆ†å‰²ã®é–¾å€¤ã‚¿ã‚¤ãƒ—
    breakpoint_threshold_amount=95           # æ„å‘³çš„ãªå¤‰åŒ–ã®é–¾å€¤
)

texts = semantic_splitter.split_documents(documents)
```

---

## 3. ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ï¼šæ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã®é¸æŠ

### ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®æ¯”è¼ƒ

| ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ | ç¨®é¡ | ç”¨é€” | ãƒ¡ãƒªãƒƒãƒˆ | ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ |
|--------------|------|------|---------|-----------|
| **FAISS** | ãƒ­ãƒ¼ã‚«ãƒ« | é–‹ç™ºã€å°ã€œä¸­è¦æ¨¡ | é«˜é€Ÿã€ç„¡æ–™ã€ç°¡å˜ | ã‚¹ã‚±ãƒ¼ãƒ«ã«é™ç•Œ |
| **Chroma** | ãƒ­ãƒ¼ã‚«ãƒ«/ã‚µãƒ¼ãƒãƒ¼ | é–‹ç™ºã€ä¸­è¦æ¨¡ | ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ¤œç´¢ãŒå¼·åŠ› | å¤§è¦æ¨¡ã«ã¯ä¸å‘ã |
| **Pinecone** | ã‚¯ãƒ©ã‚¦ãƒ‰ | æœ¬ç•ªç’°å¢ƒã€å¤§è¦æ¨¡ | ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ã€ãƒãƒãƒ¼ã‚¸ãƒ‰ | æœ‰æ–™ |
| **Weaviate** | ã‚ªãƒ³ãƒ—ãƒ¬/ã‚¯ãƒ©ã‚¦ãƒ‰ | ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚º | é«˜æ©Ÿèƒ½ã€æŸ”è»Ÿ | ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãŒè¤‡é›‘ |

### FAISSï¼šãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºã«æœ€é©

```python
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings

# ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
embeddings = OpenAIEmbeddings(
    model="text-embedding-3-small"  # ã‚³ã‚¹ãƒ‘ã®è‰¯ã„ãƒ¢ãƒ‡ãƒ«
)

# ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®ä½œæˆ
vectorstore = FAISS.from_documents(
    documents=texts,
    embedding=embeddings
)

# ãƒ­ãƒ¼ã‚«ãƒ«ã«ä¿å­˜
vectorstore.save_local("./faiss_index")

# å¾Œã§èª­ã¿è¾¼ã‚€
vectorstore = FAISS.load_local(
    "./faiss_index",
    embeddings,
    allow_dangerous_deserialization=True  # ä¿¡é ¼ã§ãã‚‹ã‚½ãƒ¼ã‚¹ã®å ´åˆã®ã¿
)
```

### Chromaï¼šæ°¸ç¶šåŒ–ã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ¤œç´¢ã«å¼·ã„

```python
from langchain_community.vectorstores import Chroma

# Chromaãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®ä½œæˆ
vectorstore = Chroma.from_documents(
    documents=texts,
    embedding=embeddings,
    persist_directory="./chroma_db"  # æ°¸ç¶šåŒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
)

# ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
results = vectorstore.similarity_search(
    "ä¿è¨¼æœŸé–“ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„",
    k=4,
    filter={"category": "warranty"}  # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ«ã‚¿
)
```

### ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã®é¸æŠ

```python
from langchain_openai import OpenAIEmbeddings
from langchain_community.embeddings import HuggingFaceEmbeddings

# OpenAI Embeddingsï¼ˆæ¨å¥¨ï¼‰
embeddings_openai = OpenAIEmbeddings(
    model="text-embedding-3-small",  # å°è¦æ¨¡: é€Ÿåº¦é‡è¦–
    # model="text-embedding-3-large"  # å¤§è¦æ¨¡: ç²¾åº¦é‡è¦–
)

# ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ï¼ˆç„¡æ–™ã€æ—¥æœ¬èªå¯¾å¿œï¼‰
embeddings_local = HuggingFaceEmbeddings(
    model_name="intfloat/multilingual-e5-large",
    model_kwargs={'device': 'cpu'},
    encode_kwargs={'normalize_embeddings': True}
)
```

**ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã®æ¯”è¼ƒ**:

| ãƒ¢ãƒ‡ãƒ« | æ¬¡å…ƒæ•° | ç”¨é€” | ã‚³ã‚¹ãƒˆ |
|--------|-------|------|--------|
| text-embedding-3-small | 1536 | ä¸€èˆ¬çš„ãªç”¨é€” | $0.02/1M tokens |
| text-embedding-3-large | 3072 | é«˜ç²¾åº¦ãŒå¿…è¦ | $0.13/1M tokens |
| multilingual-e5-large | 1024 | ãƒ­ãƒ¼ã‚«ãƒ«ã€æ—¥æœ¬èª | ç„¡æ–™ï¼ˆè¦GPUï¼‰ |

### æ¤œç´¢æˆ¦ç•¥ã®é¸æŠ

#### 1. Similarity Searchï¼ˆé¡ä¼¼åº¦æ¤œç´¢ï¼‰

```python
# æœ€ã‚‚é¡ä¼¼åº¦ã®é«˜ã„kå€‹ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å–å¾—
results = vectorstore.similarity_search(
    query="è£½å“ã®ä¿è¨¼æœŸé–“ã¯ï¼Ÿ",
    k=4
)

for doc in results:
    print(f"Score: {doc.metadata.get('score', 'N/A')}")
    print(f"Content: {doc.page_content}\n")
```

#### 2. Similarity Search with Scoreï¼ˆã‚¹ã‚³ã‚¢ä»˜ãï¼‰

```python
# ã‚¹ã‚³ã‚¢ä»˜ãã§æ¤œç´¢ï¼ˆé¡ä¼¼åº¦ã‚’ç¢ºèªã§ãã‚‹ï¼‰
results_with_scores = vectorstore.similarity_search_with_score(
    query="è£½å“ã®ä¿è¨¼æœŸé–“ã¯ï¼Ÿ",
    k=4
)

for doc, score in results_with_scores:
    print(f"é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢: {score:.3f}")
    print(f"å†…å®¹: {doc.page_content[:100]}...\n")
```

#### 3. MMRï¼ˆMaximal Marginal Relevanceï¼‰

å¤šæ§˜æ€§ã‚’è€ƒæ…®ã—ãŸæ¤œç´¢ï¼š

```python
# MMR: é¡ä¼¼åº¦ã¨å¤šæ§˜æ€§ã®ãƒãƒ©ãƒ³ã‚¹
results = vectorstore.max_marginal_relevance_search(
    query="è£½å“ã®ä¿è¨¼æœŸé–“ã¯ï¼Ÿ",
    k=4,
    fetch_k=20,              # ã¾ãš20å€‹ã®å€™è£œã‚’å–å¾—
    lambda_mult=0.5          # å¤šæ§˜æ€§ã®é‡ã¿ï¼ˆ0=å¤šæ§˜æ€§é‡è¦–ã€1=é¡ä¼¼åº¦é‡è¦–ï¼‰
)
```

#### 4. é¡ä¼¼åº¦é–¾å€¤ã«ã‚ˆã‚‹çµã‚Šè¾¼ã¿

```python
# ä¸€å®šä»¥ä¸Šã®é¡ä¼¼åº¦ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ã¿å–å¾—
retriever = vectorstore.as_retriever(
    search_type="similarity_score_threshold",
    search_kwargs={
        "score_threshold": 0.8,  # é¡ä¼¼åº¦ã®é–¾å€¤
        "k": 4
    }
)

results = retriever.get_relevant_documents("è£½å“ã®ä¿è¨¼æœŸé–“ã¯ï¼Ÿ")
```

---

## 4. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­è¨ˆï¼šRAGã®ç²¾åº¦ã‚’é«˜ã‚ã‚‹

ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­è¨ˆã¯RAGã‚·ã‚¹ãƒ†ãƒ ã®å“è³ªã«ç›´æ¥å½±éŸ¿ã—ã¾ã™ã€‚

### åŸºæœ¬çš„ãªRAGãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ

```python
from langchain.prompts import ChatPromptTemplate

# RAGç”¨ã®åŸºæœ¬ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
rag_template = """ã‚ãªãŸã¯ç¤¾å†…ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½¿ç”¨ã—ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«æ­£ç¢ºã«ç­”ãˆã¦ãã ã•ã„ã€‚

é‡è¦ãªãƒ«ãƒ¼ãƒ«:
1. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«åŸºã¥ã„ã¦ã®ã¿å›ç­”ã—ã¦ãã ã•ã„
2. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«æƒ…å ±ãŒãªã„å ´åˆã¯ã€Œæä¾›ã•ã‚ŒãŸæƒ…å ±ã§ã¯å›ç­”ã§ãã¾ã›ã‚“ã€ã¨ç­”ãˆã¦ãã ã•ã„
3. æ¨æ¸¬ã‚„ä¸€èˆ¬çŸ¥è­˜ã§å›ç­”ã—ãªã„ã§ãã ã•ã„
4. å›ç­”ã«ã¯å‡ºå…¸ï¼ˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåï¼‰ã‚’æ˜è¨˜ã—ã¦ãã ã•ã„

ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ:
{context}

è³ªå•: {question}

å›ç­”:"""

prompt = ChatPromptTemplate.from_template(rag_template)
```

### å‡ºå…¸å¼•ç”¨ã‚’å«ã‚€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ

```python
rag_template_with_sources = """ã‚ãªãŸã¯ç¤¾å†…ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½¿ç”¨ã—ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«æ­£ç¢ºã«ç­”ãˆã¦ãã ã•ã„ã€‚

ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ:
{context}

è³ªå•: {question}

å›ç­”ã™ã‚‹éš›ã¯ã€ä»¥ä¸‹ã®å½¢å¼ã«å¾“ã£ã¦ãã ã•ã„:

ã€å›ç­”ã€‘
ã“ã“ã«è³ªå•ã¸ã®å›ç­”ã‚’è¨˜è¿°

ã€å‡ºå…¸ã€‘
- ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå1ï¼ˆè©²å½“ç®‡æ‰€ï¼‰
- ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå2ï¼ˆè©²å½“ç®‡æ‰€ï¼‰

ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«æƒ…å ±ãŒãªã„å ´åˆã¯ã€ã€Œæä¾›ã•ã‚ŒãŸæƒ…å ±ã§ã¯å›ç­”ã§ãã¾ã›ã‚“ã€ã¨ç­”ãˆã¦ãã ã•ã„ã€‚
"""

prompt = ChatPromptTemplate.from_template(rag_template_with_sources)
```

### Few-shotãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ

è‰¯ã„å›ç­”ã®ä¾‹ã‚’ç¤ºã™ã“ã¨ã§ã€å“è³ªã‚’å‘ä¸Šã•ã›ã¾ã™ï¼š

```python
few_shot_template = """ã‚ãªãŸã¯ç¤¾å†…ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®ä¾‹ã‚’å‚è€ƒã«ã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«åŸºã¥ã„ã¦è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚

ã€ä¾‹1ã€‘
ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ: å¹´æ¬¡æœ‰çµ¦ä¼‘æš‡ã¯ã€å…¥ç¤¾æ—¥ã‹ã‚‰6ãƒ¶æœˆçµŒéå¾Œã«10æ—¥é–“ä»˜ä¸ã•ã‚Œã¾ã™ã€‚
è³ªå•: æœ‰çµ¦ä¼‘æš‡ã¯ã„ã¤ã‚‚ã‚‰ãˆã¾ã™ã‹ï¼Ÿ
å›ç­”: å…¥ç¤¾æ—¥ã‹ã‚‰6ãƒ¶æœˆçµŒéå¾Œã«10æ—¥é–“ã®å¹´æ¬¡æœ‰çµ¦ä¼‘æš‡ãŒä»˜ä¸ã•ã‚Œã¾ã™ã€‚
å‡ºå…¸: å°±æ¥­è¦å‰‡ç¬¬15æ¡

ã€ä¾‹2ã€‘
ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆAã®ç´æœŸã¯2024å¹´3æœˆ31æ—¥ã§ã™ã€‚
è³ªå•: ã‚·ã‚¹ãƒ†ãƒ ã®ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¦ä»¶ã¯ï¼Ÿ
å›ç­”: æä¾›ã•ã‚ŒãŸæƒ…å ±ã§ã¯ã€ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¦ä»¶ã«ã¤ã„ã¦ã¯å›ç­”ã§ãã¾ã›ã‚“ã€‚

---

ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ:
{context}

è³ªå•: {question}

å›ç­”:"""

prompt = ChatPromptTemplate.from_template(few_shot_template)
```

### æ®µéšçš„æ¨è«–ã‚’ä¿ƒã™ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆChain-of-Thoughtï¼‰

```python
cot_template = """ã‚ãªãŸã¯ç¤¾å†…ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚

ä»¥ä¸‹ã®ã‚¹ãƒ†ãƒƒãƒ—ã§å›ç­”ã‚’æ§‹ç¯‰ã—ã¦ãã ã•ã„:

1. ã¾ãšã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰è³ªå•ã«é–¢é€£ã™ã‚‹æƒ…å ±ã‚’ç‰¹å®š
2. ãã®æƒ…å ±ã‚’æ•´ç†ã—ã¦è«–ç†çš„ã«çµ„ã¿ç«‹ã¦ã‚‹
3. æœ€çµ‚çš„ãªå›ç­”ã‚’ç”Ÿæˆ

ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ:
{context}

è³ªå•: {question}

ã€ã‚¹ãƒ†ãƒƒãƒ—1: é–¢é€£æƒ…å ±ã®ç‰¹å®šã€‘
ï¼ˆã“ã“ã«ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æŠ½å‡ºã—ãŸé–¢é€£æƒ…å ±ã‚’è¨˜è¿°ï¼‰

ã€ã‚¹ãƒ†ãƒƒãƒ—2: æƒ…å ±ã®æ•´ç†ã€‘
ï¼ˆã“ã“ã«æƒ…å ±ã‚’è«–ç†çš„ã«æ•´ç†ï¼‰

ã€ã‚¹ãƒ†ãƒƒãƒ—3: æœ€çµ‚å›ç­”ã€‘
ï¼ˆã“ã“ã«æœ€çµ‚çš„ãªå›ç­”ï¼‰

ã€å‡ºå…¸ã€‘
ï¼ˆå‚ç…§ã—ãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼‰
"""

prompt = ChatPromptTemplate.from_template(cot_template)
```

### è³ªå•ã®æ›–æ˜§ã•ã‚’è§£æ¶ˆã™ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ

```python
clarification_template = """ã‚ãªãŸã¯ç¤¾å†…ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚

ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ãŒæ›–æ˜§ãªå ´åˆã¯ã€æ˜ç¢ºåŒ–ã®ãŸã‚ã®è³ªå•ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚
è³ªå•ãŒæ˜ç¢ºãªå ´åˆã¯ã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«åŸºã¥ã„ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚

ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ:
{context}

è³ªå•: {question}

è³ªå•ãŒæ›–æ˜§ãªå ´åˆã®å¯¾å¿œä¾‹:
- ã€Œè£½å“ã€ã¨è¨€åŠã•ã‚Œã¦ã„ã‚‹ãŒã€è¤‡æ•°ã®è£½å“ãŒã‚ã‚‹ â†’ ã©ã®è£½å“ã«ã¤ã„ã¦çŸ¥ã‚ŠãŸã„ã‹ç¢ºèª
- æ™‚æœŸãŒä¸æ˜ç¢º â†’ ã„ã¤ã®æ™‚ç‚¹ã®æƒ…å ±ãŒå¿…è¦ã‹ç¢ºèª
- è¤‡æ•°ã®è§£é‡ˆãŒå¯èƒ½ â†’ å…·ä½“çš„ã«ä½•ã‚’çŸ¥ã‚ŠãŸã„ã‹ç¢ºèª

å›ç­”:"""

prompt = ChatPromptTemplate.from_template(clarification_template)
```

---

## 5. å®Œå…¨ãªå®Ÿè£…ï¼šã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰

ã™ã¹ã¦ã‚’çµ±åˆã—ãŸRAGã‚·ã‚¹ãƒ†ãƒ ã®å®Œå…¨å®Ÿè£…ï¼š

```python
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.chains import RetrievalQA
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain
import os

class DocumentSearchSystem:
    """ç¤¾å†…ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self, documents_path, index_path="./faiss_index"):
        self.documents_path = documents_path
        self.index_path = index_path
        self.vectorstore = None
        self.qa_chain = None

        # ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
        self.embeddings = OpenAIEmbeddings(
            model="text-embedding-3-small"
        )

        # LLMã®åˆæœŸåŒ–
        self.llm = ChatOpenAI(
            model="gpt-4",
            temperature=0,  # å†ç¾æ€§ã®ãŸã‚0ã«è¨­å®š
        )

    def load_documents(self):
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®èª­ã¿è¾¼ã¿"""
        print("ğŸ“„ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™...")

        loader = DirectoryLoader(
            self.documents_path,
            glob="**/*.md",
            loader_cls=TextLoader,
            loader_kwargs={'encoding': 'utf-8'}
        )
        documents = loader.load()

        print(f"âœ… {len(documents)}ä»¶ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
        return documents

    def split_documents(self, documents):
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²"""
        print("âœ‚ï¸  ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’åˆ†å‰²ã—ã¦ã„ã¾ã™...")

        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            separators=["\n\n", "\n", "ã€‚", "ã€", " ", ""]
        )

        texts = text_splitter.split_documents(documents)
        print(f"âœ… {len(texts)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã—ã¾ã—ãŸ")
        return texts

    def create_vectorstore(self, texts, force_rebuild=False):
        """ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®ä½œæˆ"""
        if os.path.exists(self.index_path) and not force_rebuild:
            print("ğŸ“¦ æ—¢å­˜ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™...")
            self.vectorstore = FAISS.load_local(
                self.index_path,
                self.embeddings,
                allow_dangerous_deserialization=True
            )
            print("âœ… ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
        else:
            print("ğŸ”¨ ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ä½œæˆã—ã¦ã„ã¾ã™...")
            self.vectorstore = FAISS.from_documents(
                texts,
                self.embeddings
            )
            self.vectorstore.save_local(self.index_path)
            print("âœ… ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ä½œæˆãƒ»ä¿å­˜ã—ã¾ã—ãŸ")

        return self.vectorstore

    def setup_qa_chain(self):
        """QAãƒã‚§ãƒ¼ãƒ³ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        print("âš™ï¸  QAãƒã‚§ãƒ¼ãƒ³ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã—ã¦ã„ã¾ã™...")

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®å®šç¾©
        template = """ã‚ãªãŸã¯ç¤¾å†…ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½¿ç”¨ã—ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«æ­£ç¢ºã«ç­”ãˆã¦ãã ã•ã„ã€‚

é‡è¦ãªãƒ«ãƒ¼ãƒ«:
1. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«åŸºã¥ã„ã¦ã®ã¿å›ç­”ã—ã¦ãã ã•ã„
2. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«æƒ…å ±ãŒãªã„å ´åˆã¯ã€Œæä¾›ã•ã‚ŒãŸæƒ…å ±ã§ã¯å›ç­”ã§ãã¾ã›ã‚“ã€ã¨æ˜ç¢ºã«ç­”ãˆã¦ãã ã•ã„
3. æ¨æ¸¬ã‚„ä¸€èˆ¬çŸ¥è­˜ã§å›ç­”ã—ãªã„ã§ãã ã•ã„
4. å›ç­”ã«ã¯å¿…ãšå‡ºå…¸ï¼ˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåã‚„ãƒšãƒ¼ã‚¸ï¼‰ã‚’æ˜è¨˜ã—ã¦ãã ã•ã„
5. å›ç­”ã¯ç°¡æ½”ã‹ã¤æ­£ç¢ºã«

ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ:
{context}

è³ªå•: {input}

å›ç­”:"""

        prompt = ChatPromptTemplate.from_template(template)

        # Retrieverã®è¨­å®šï¼ˆMMRã§å¤šæ§˜æ€§ã‚’è€ƒæ…®ï¼‰
        retriever = self.vectorstore.as_retriever(
            search_type="mmr",
            search_kwargs={
                "k": 4,           # å–å¾—ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°
                "fetch_k": 20,    # MMRç”¨ã®å€™è£œæ•°
                "lambda_mult": 0.7  # é¡ä¼¼åº¦vså¤šæ§˜æ€§ã®ãƒãƒ©ãƒ³ã‚¹
            }
        )

        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆçµåˆãƒã‚§ãƒ¼ãƒ³ã®ä½œæˆ
        document_chain = create_stuff_documents_chain(
            self.llm,
            prompt
        )

        # Retrievalãƒã‚§ãƒ¼ãƒ³ã®ä½œæˆ
        self.qa_chain = create_retrieval_chain(
            retriever,
            document_chain
        )

        print("âœ… QAãƒã‚§ãƒ¼ãƒ³ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãŒå®Œäº†ã—ã¾ã—ãŸ")
        return self.qa_chain

    def initialize(self, force_rebuild=False):
        """ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–"""
        documents = self.load_documents()
        texts = self.split_documents(documents)
        self.create_vectorstore(texts, force_rebuild)
        self.setup_qa_chain()
        print("ğŸš€ ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–ãŒå®Œäº†ã—ã¾ã—ãŸ\n")

    def query(self, question):
        """è³ªå•ã«å¯¾ã™ã‚‹å›ç­”ã‚’ç”Ÿæˆ"""
        if not self.qa_chain:
            raise ValueError("ã‚·ã‚¹ãƒ†ãƒ ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚initialize()ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")

        print(f"ğŸ” è³ªå•: {question}")
        print("ğŸ’­ å›ç­”ã‚’ç”Ÿæˆã—ã¦ã„ã¾ã™...\n")

        result = self.qa_chain.invoke({"input": question})

        print("ğŸ“ å›ç­”:")
        print(result['answer'])
        print("\nğŸ“š å‚ç…§ã—ãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ:")
        for i, doc in enumerate(result['context'], 1):
            source = doc.metadata.get('source', 'ä¸æ˜')
            print(f"  {i}. {source}")
            print(f"     å†…å®¹: {doc.page_content[:100]}...")
        print()

        return result

    def search_similar_documents(self, query, k=5):
        """é¡ä¼¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®æ¤œç´¢"""
        if not self.vectorstore:
            raise ValueError("ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ãŒä½œæˆã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")

        docs_with_scores = self.vectorstore.similarity_search_with_score(
            query, k=k
        )

        print(f"ğŸ” ã€Œ{query}ã€ã«é–¢é€£ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ:\n")
        for i, (doc, score) in enumerate(docs_with_scores, 1):
            print(f"{i}. ã‚¹ã‚³ã‚¢: {score:.3f}")
            print(f"   ã‚½ãƒ¼ã‚¹: {doc.metadata.get('source', 'ä¸æ˜')}")
            print(f"   å†…å®¹: {doc.page_content[:150]}...")
            print()

        return docs_with_scores


# ===========================
# ä½¿ç”¨ä¾‹
# ===========================

if __name__ == "__main__":
    # ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–
    system = DocumentSearchSystem(
        documents_path="./knowledge_base",
        index_path="./faiss_index"
    )

    # åˆå›å®Ÿè¡Œæ™‚ã¯force_rebuild=Trueã‚’æŒ‡å®š
    system.initialize(force_rebuild=False)

    # è³ªå•å¿œç­”
    questions = [
        "è£½å“ã®ä¿è¨¼æœŸé–“ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„",
        "å¹´æ¬¡æœ‰çµ¦ä¼‘æš‡ã¯ä½•æ—¥ä»˜ä¸ã•ã‚Œã¾ã™ã‹ï¼Ÿ",
        "ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒãƒªã‚·ãƒ¼ã®ä¸»ãªå†…å®¹ã¯ä½•ã§ã™ã‹ï¼Ÿ"
    ]

    for question in questions:
        result = system.query(question)
        print("-" * 80)
```

---

## 6. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

### ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã®ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°

```python
from langchain.evaluation import load_evaluator
from langchain_openai import ChatOpenAI

def evaluate_chunk_size(documents, chunk_sizes=[500, 1000, 1500, 2000]):
    """ç•°ãªã‚‹ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã§ã®æ€§èƒ½è©•ä¾¡"""
    results = {}

    test_questions = [
        "è£½å“ã®ä¿è¨¼æœŸé–“ã¯ï¼Ÿ",
        "è¿”å“ãƒãƒªã‚·ãƒ¼ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„",
        # ... more test questions
    ]

    for chunk_size in chunk_sizes:
        print(f"\nğŸ“Š ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º: {chunk_size}")

        # Text Splitterã®ä½œæˆ
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=int(chunk_size * 0.2)
        )

        texts = text_splitter.split_documents(documents)

        # ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®ä½œæˆ
        vectorstore = FAISS.from_documents(texts, embeddings)

        # æ¤œç´¢æ€§èƒ½ã®è©•ä¾¡
        # ... evaluation logic

        results[chunk_size] = {
            'num_chunks': len(texts),
            'avg_chunk_length': sum(len(t.page_content) for t in texts) / len(texts),
            # ... other metrics
        }

    return results
```

### æ¤œç´¢çµæœã®å†ãƒ©ãƒ³ã‚­ãƒ³ã‚°

æ¤œç´¢çµæœã®å“è³ªã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã€å†ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’å®Ÿæ–½ï¼š

```python
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor

def create_reranking_retriever(vectorstore, llm):
    """å†ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’è¡Œã†Retrieverã‚’ä½œæˆ"""

    # ãƒ™ãƒ¼ã‚¹ã®Retriever
    base_retriever = vectorstore.as_retriever(
        search_kwargs={"k": 10}  # å¤šã‚ã«å–å¾—
    )

    # LLMã‚’ä½¿ã£ãŸåœ§ç¸®ãƒ»å†ãƒ©ãƒ³ã‚­ãƒ³ã‚°
    compressor = LLMChainExtractor.from_llm(llm)

    compression_retriever = ContextualCompressionRetriever(
        base_compressor=compressor,
        base_retriever=base_retriever
    )

    return compression_retriever

# ä½¿ç”¨ä¾‹
reranking_retriever = create_reranking_retriever(vectorstore, llm)
docs = reranking_retriever.get_relevant_documents("è£½å“ã®ä¿è¨¼æœŸé–“ã¯ï¼Ÿ")
```

### ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°ã®å®Ÿè£…

```python
from langchain.cache import InMemoryCache
from langchain.globals import set_llm_cache

# ã‚¤ãƒ³ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®æœ‰åŠ¹åŒ–
set_llm_cache(InMemoryCache())

# ã“ã‚Œã«ã‚ˆã‚Šã€åŒã˜è³ªå•ã«å¯¾ã—ã¦ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰çµæœã‚’è¿”ã™
result1 = qa_chain.invoke({"input": "è£½å“ã®ä¿è¨¼æœŸé–“ã¯ï¼Ÿ"})  # LLMå‘¼ã³å‡ºã—
result2 = qa_chain.invoke({"input": "è£½å“ã®ä¿è¨¼æœŸé–“ã¯ï¼Ÿ"})  # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—
```

### ãƒãƒƒãƒå‡¦ç†ã«ã‚ˆã‚‹åŠ¹ç‡åŒ–

```python
async def batch_query(system, questions):
    """è¤‡æ•°ã®è³ªå•ã‚’ä¸¦åˆ—å‡¦ç†"""
    import asyncio

    async def query_async(question):
        return await system.qa_chain.ainvoke({"input": question})

    tasks = [query_async(q) for q in questions]
    results = await asyncio.gather(*tasks)

    return results

# ä½¿ç”¨ä¾‹
import asyncio

questions = [
    "è£½å“ã®ä¿è¨¼æœŸé–“ã¯ï¼Ÿ",
    "è¿”å“ãƒãƒªã‚·ãƒ¼ã¯ï¼Ÿ",
    "å–¶æ¥­æ™‚é–“ã¯ï¼Ÿ"
]

results = asyncio.run(batch_query(system, questions))
```

---

## 7. å®Ÿé‹ç”¨ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

### ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

```python
def safe_query(system, question, max_retries=3):
    """ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä»˜ãã®è³ªå•å‡¦ç†"""
    import time

    for attempt in range(max_retries):
        try:
            result = system.query(question)
            return result
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸï¼ˆè©¦è¡Œ {attempt + 1}/{max_retries}ï¼‰: {e}")

            if attempt < max_retries - 1:
                wait_time = 2 ** attempt  # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•
                print(f"â³ {wait_time}ç§’å¾…æ©Ÿã—ã¦ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™...")
                time.sleep(wait_time)
            else:
                print("âŒ æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°ã«é”ã—ã¾ã—ãŸ")
                return {
                    'answer': 'ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸãŸã‚å›ç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚',
                    'context': [],
                    'error': str(e)
                }
```

### å›ç­”å“è³ªã®ç›£è¦–

```python
from langsmith import Client

def log_query_to_langsmith(question, answer, context, feedback=None):
    """LangSmithã«è³ªå•ã¨å›ç­”ã‚’ãƒ­ã‚°"""
    client = Client()

    # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã®è¨˜éŒ²
    run_id = client.create_run(
        name="qa-query",
        inputs={"question": question},
        outputs={"answer": answer, "context": context},
        project_name="rag-system"
    )

    if feedback:
        client.create_feedback(
            run_id=run_id,
            key="user-rating",
            score=feedback['score'],
            comment=feedback.get('comment', '')
        )
```

### ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å¢—åˆ†æ›´æ–°

```python
def update_vectorstore(system, new_documents):
    """ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã«æ–°ã—ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’è¿½åŠ """

    # æ–°ã—ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®åˆ†å‰²
    texts = system.split_documents(new_documents)

    # æ—¢å­˜ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã«è¿½åŠ 
    system.vectorstore.add_documents(texts)

    # æ°¸ç¶šåŒ–
    system.vectorstore.save_local(system.index_path)

    print(f"âœ… {len(texts)}å€‹ã®æ–°ã—ã„ãƒãƒ£ãƒ³ã‚¯ã‚’è¿½åŠ ã—ã¾ã—ãŸ")
```

### ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ«ã‚¿ã®æ´»ç”¨

```python
# ã‚«ãƒ†ã‚´ãƒªã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
retriever = vectorstore.as_retriever(
    search_kwargs={
        "k": 4,
        "filter": {"category": "warranty"}  # ä¿è¨¼é–¢é€£ã®ã¿
    }
)

# æ—¥ä»˜ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
from datetime import datetime, timedelta

recent_date = (datetime.now() - timedelta(days=30)).isoformat()
retriever = vectorstore.as_retriever(
    search_kwargs={
        "k": 4,
        "filter": {
            "last_updated": {"$gte": recent_date}  # 30æ—¥ä»¥å†…ã®æ–‡æ›¸ã®ã¿
        }
    }
)
```

---

## 8. ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### å›ç­”ãŒä¸æ­£ç¢ºãªå ´åˆ

**åŸå› ã¨å¯¾ç­–**:

1. **ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºãŒä¸é©åˆ‡**
   - å°ã•ã™ãã‚‹ â†’ æ–‡è„ˆãŒä¸è¶³
   - å¤§ãã™ãã‚‹ â†’ ãƒã‚¤ã‚ºãŒå¤šã„
   - å¯¾ç­–: chunk_sizeã‚’500-1500ã®ç¯„å›²ã§èª¿æ•´

2. **æ¤œç´¢çµæœãŒé–¢é€£æ€§ãŒä½ã„**
   - ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã‚’å¤‰æ›´ï¼ˆtext-embedding-3-largeï¼‰
   - MMRã§kã¨fetch_kã‚’èª¿æ•´
   - ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ«ã‚¿ã‚’æ´»ç”¨

3. **ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒä¸é©åˆ‡**
   - Few-shotã®ä¾‹ã‚’è¿½åŠ 
   - å›ç­”å½¢å¼ã‚’æ˜ç¢ºã«æŒ‡å®š
   - ã€Œã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«åŸºã¥ã„ã¦ã®ã¿ã€ã‚’å¼·èª¿

### ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒå¤šã„å ´åˆ

```python
# FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æœ€é©åŒ–
vectorstore.index.add_with_ids()  # IDã‚’ä½¿ã£ãŸåŠ¹ç‡çš„ãªè¿½åŠ 

# ã¾ãŸã¯ã€ã‚ˆã‚Šå°ã•ã„ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
embeddings = OpenAIEmbeddings(
    model="text-embedding-3-small",  # largeã®ä»£ã‚ã‚Šã«small
    dimensions=512  # æ¬¡å…ƒæ•°ã‚’å‰Šæ¸›
)
```

### å¿œç­”ãŒé…ã„å ´åˆ

1. **ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°ã®æœ‰åŠ¹åŒ–**: ä¸Šè¨˜ã®ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°å®Ÿè£…ã‚’å‚ç…§
2. **æ¤œç´¢çµæœã®æ•°ã‚’å‰Šæ¸›**: kã‚’4ä»¥ä¸‹ã«è¨­å®š
3. **ã‚ˆã‚Šå°ã•ã„LLMã‚’ä½¿ç”¨**: gpt-4ã®ä»£ã‚ã‚Šã«gpt-3.5-turbo
4. **ä¸¦åˆ—å‡¦ç†ã®å®Ÿè£…**: batch_queryé–¢æ•°ã‚’ä½¿ç”¨

---

## ã¾ã¨ã‚

ã“ã®è¨˜äº‹ã§ã¯ã€LangChainã‚’ä½¿ã£ãŸç¤¾å†…ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ï¼ˆRAGï¼‰ã®æ§‹ç¯‰æ–¹æ³•ã‚’ã€å®Ÿè£…ã®è©³ç´°ã¾ã§è¸ã¿è¾¼ã‚“ã§è§£èª¬ã—ã¾ã—ãŸã€‚

### é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ

**ãƒãƒ£ãƒ³ã‚¯æˆ¦ç•¥**:
- RecursiveCharacterTextSplitterã‚’ä½¿ç”¨
- chunk_size=1000ã€chunk_overlap=200ãŒæ¨™æº–çš„
- æ—¥æœ¬èªã§ã¯å¥ç‚¹ï¼ˆã€‚ï¼‰ã‚’åŒºåˆ‡ã‚Šæ–‡å­—ã«è¿½åŠ 

**ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢**:
- é–‹ç™º/å°è¦æ¨¡: FAISS
- æœ¬ç•ª/å¤§è¦æ¨¡: Pinecone
- text-embedding-3-smallãŒã‚³ã‚¹ãƒ‘è‰¯å¥½

**ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­è¨ˆ**:
- ã€Œã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«åŸºã¥ã„ã¦ã®ã¿å›ç­”ã€ã‚’æ˜è¨˜
- å‡ºå…¸ã®å¼•ç”¨ã‚’å¿…é ˆåŒ–
- Few-shotã§è‰¯ã„å›ç­”ä¾‹ã‚’æç¤º

**ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹**:
- ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°ã‚’æœ‰åŠ¹åŒ–
- MMRã§æ¤œç´¢å“è³ªã‚’å‘ä¸Š
- LangSmithã§ç¶™ç¶šçš„ãªå“è³ªç›£è¦–

### æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

1. **ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ©Ÿèƒ½ã®è¿½åŠ **: è¤‡æ•°ã®ãƒ„ãƒ¼ãƒ«ã‚’çµ„ã¿åˆã‚ã›ãŸé«˜åº¦ãªæ¤œç´¢
2. **ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å¯¾å¿œ**: ç”»åƒã‚„è¡¨ã‚’å«ã‚€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å‡¦ç†
3. **ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢**: ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã®çµ„ã¿åˆã‚ã›
4. **ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°**: ç‰¹å®šãƒ‰ãƒ¡ã‚¤ãƒ³ã«æœ€é©åŒ–ã—ãŸã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«

RAGã‚·ã‚¹ãƒ†ãƒ ã®å“è³ªã¯ã€ãƒ‡ãƒ¼ã‚¿ã®å“è³ªã€ãƒãƒ£ãƒ³ã‚¯æˆ¦ç•¥ã€æ¤œç´¢ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­è¨ˆã®çµ„ã¿åˆã‚ã›ã§æ±ºã¾ã‚Šã¾ã™ã€‚ã“ã®è¨˜äº‹ã§ç´¹ä»‹ã—ãŸãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã‚’åŸºã«ã€è‡ªç¤¾ã®ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ã«æœ€é©åŒ–ã—ã¦ã„ãã¾ã—ã‚‡ã†ï¼

---

## å‚è€ƒãƒªãƒ³ã‚¯

- [LangChainå…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://python.langchain.com/)
- [LangChain RAGãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«](https://python.langchain.com/docs/use_cases/question_answering/)
- [OpenAI Embeddings](https://platform.openai.com/docs/guides/embeddings)
- [FAISS Documentation](https://github.com/facebookresearch/faiss)
- [LangSmith](https://smith.langchain.com/)
